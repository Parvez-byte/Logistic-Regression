{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Logistic Regression"
      ],
      "metadata": {
        "id": "vwsXxirOdCy0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. What is Logistic Regression, and How does it differ from Linear Regression?\n",
        "- Logistic Regression is a supervised machine learning algorithm used for classification tasks. It is especially useful for binary classification, where the target variable has only two possible outcomes—such as 0 or 1, True or False, Yes or No.\n",
        "- Instead of predicting a continuous value (like Linear Regression), Logistic Regression predicts the probability that a given input belongs to a particular class. It uses the Sigmoid function to convert the linear combination of inputs into a probability between 0 and 1.\n",
        "\n",
        "- Differences between Logistic Regression and Linear Regression >>\n",
        "\n",
        "- Problem Type\n",
        "  - Logistic Regression: Used for classification problems.\n",
        "  - Linear Regression: Used for regression (predicting continuous values).\n",
        "\n",
        "- Output Values\n",
        "  - Logistic Regression: Outputs probabilities (values between 0 and 1).\n",
        "  - Linear Regression: Outputs real continuous values (can range from -∞ to +∞).\n",
        "\n",
        "- Function Used\n",
        "  - Logistic Regression: Uses the sigmoid function to squash output.\n",
        "  - Linear Regression: Uses a linear equation directly.\n",
        "\n",
        "-  Curve Type\n",
        "   - Logistic Regression: Produces an S-shaped curve (non-linear).\n",
        "   - Linear Regression: Produces a straight line.\n",
        "\n",
        "- Decision Boundary\n",
        "  - Logistic Regression: Classifies output into classes (e.g., if probability > 0.5 → class 1).\n",
        "  - Linear Regression: Does not have a concept of classification or threshold.\n",
        "\n",
        "- Loss Function\n",
        "  - Logistic Regression: Uses Log Loss (Binary Cross-Entropy).\n",
        "  - Linear Regression: Uses Mean Squared Error (MSE).\n",
        "\n",
        "- Interpretation\n",
        "  - Logistic Regression: Interprets output as probability of a class.\n",
        "  - Linear Regression: Interprets output as a predicted value."
      ],
      "metadata": {
        "id": "6umL6DwTdC2R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. What is the Mathematical Equation of Logistic Regression?\n",
        "- Logistic Regression is built using two main parts:\n",
        "\n",
        "- PART -1\n",
        "   - Linear Equation (same as linear regression):\n",
        "             z = b0 + b1x1 + b2x2 + ... + bn*xn\n",
        "             Where:\n",
        "             z is the linear combination of input features\n",
        "             b0 is the intercept\n",
        "             b1, b2, ..., bn are coefficients (weights)\n",
        "             x1, x2, ..., xn are input feature values\n",
        "\n",
        "- PART - 2\n",
        "  - Sigmoid Function (to convert z into a probability):\n",
        "            h(z) = 1 / (1 + e^(-z))\n",
        "\n",
        "  - This function transforms any real number z into a value between 0 and 1. This is useful for interpreting the output as a probability.\n",
        "\n",
        "- Final Logistic Regression Equation:\n",
        "         P(Y = 1 | X) = 1 / (1 + e^(-(b0 + b1x1 + b2x2 + ... + bn*xn)))\n",
        "         Or simply:\n",
        "         P(Y = 1 | X) = 1 / (1 + e^(-z))\n",
        "\n",
        "- The output of logistic regression is the probability that the dependent variable Y is equal to 1, given the input features X.\n",
        "- This probability is then used for classification:\n",
        "  - If probability ≥ 0.5 → Predict class 1\n",
        "  - If probability < 0.5 → Predict class 0\n",
        "\n"
      ],
      "metadata": {
        "id": "F_nPhpPbdC4_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Why do we use the Sigmoid Function in Logistic Regression?\n",
        "- The Sigmoid function is used in logistic regression to convert the linear output (which can be any real number) into a probability between 0 and 1. Since logistic regression is used for classification, we need a way to express the likelihood that a given input belongs to a certain class — and probabilities must always be between 0 and 1.\n",
        "\n",
        "Definition of Sigmoid Function:\n",
        "          \n",
        "          sigmoid(z) = 1 / (1 + e^(-z))\n",
        "          Where:\n",
        "          z is the linear combination of input features (z = b0 + b1x1 + b2x2 + ... + bn*xn)\n",
        "          e is the base of the natural logarithm (approx. 2.718)\n",
        "\n",
        "- Reasons Why We Use the Sigmoid Function:\n",
        "\n",
        "- Probability Output - The Sigmoid function maps any real-valued number to a value between 0 and 1, which is perfect for representing a probability. Probabilities are required in classification problems because they provide a clear measure of certainty about the predicted class.\n",
        "\n",
        "- Binary Classification - Logistic Regression is typically used for binary classification where the output is either 0 or 1. The Sigmoid function helps by outputting probabilities, which can then be thresholded (e.g., 0.5) to decide between two classes (class 0 or class 1).\n",
        "\n",
        "- Smooth Gradient - The Sigmoid function has a smooth and continuous curve, making its derivative easy to compute. This smoothness is crucial because it helps optimization algorithms like gradient descent work effectively and converge faster during model training.\n",
        "\n",
        "- Threshold-Based Decision - Once the Sigmoid function calculates the probability, we can easily classify the result by applying a threshold. If the output probability is greater than or equal to 0.5, we predict class 1; if less than 0.5, we predict class 0, making the decision clear and interpretable.\n",
        "\n",
        "- Mathematical Convenience - The derivative of the Sigmoid function is simple and easy to compute, which makes it computationally efficient for backpropagation during training. This efficiency is critical when working with large datasets, ensuring the training process is fast and accurate.\n"
      ],
      "metadata": {
        "id": "j5hWvz9_dC70"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. What is the Cost Function of Logistic Regression?\n",
        "- In Logistic Regression, the cost function measures how well the model's predictions match the actual labels. It is used to guide the optimization process, helping the model minimize errors and improve accuracy.\n",
        "\n",
        "- Binary Cross-Entropy Loss (Log Loss)\n",
        " - The cost function used in Logistic Regression is called Binary Cross-Entropy Loss (also known as Log Loss). It calculates the difference between the actual class labels and the predicted probabilities, penalizing wrong predictions more severely.\n",
        "          The formula for the cost function is:\n",
        "          J(θ) = - (1/m) * Σ [ y(i) * log(h(x(i))) + (1 - y(i)) * log(1 - h(x(i))) ]\n",
        "          Where:\n",
        "          m is the number of training examples\n",
        "          y(i) is the actual label (either 0 or 1)\n",
        "          h(x(i)) is the predicted probability for the i-th training example\n",
        "          log is the natural logarithm\n",
        "\n",
        "- Why is it called Cross-Entropy?\n",
        "  - The term cross-entropy comes from information theory and is used to measure the difference between two probability distributions. In the case of logistic regression, it measures how different the predicted probabilities are from the actual labels. The smaller the difference, the lower the cost.\n",
        "\n",
        "- Interpretation of the Cost Function\n",
        "  - When the model makes a correct prediction (close to 1 for class 1 or close to 0 for class 0), the cost is small.\n",
        "  - When the model makes an incorrect prediction, the cost increases. If the predicted probability is far from the actual class, the cost becomes very large.\n",
        "\n",
        "- This cost function is used because it penalizes wrong predictions more heavily as the predicted probability moves further from the actual label. This helps in optimizing the model to improve its accuracy. Moreover, it's convex, which means the optimization process (e.g., using gradient descent) will converge to a global minimum, ensuring that the model can find the optimal set of weights."
      ],
      "metadata": {
        "id": "JSfMAly_dC_c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. What is Regularization in Logistic Regression? Why is it Needed?\n",
        "- Regularization in logistic regression is a technique used to prevent overfitting by adding a penalty term to the cost function. It discourages the model from becoming too complex and ensures it generalizes well to new data.\n",
        "\n",
        "- What is Regularization in Logistic Regression?\n",
        "  - Regularization adds a penalty to the cost function to reduce model complexity. It can be of two types:\n",
        "   - L1 regularization (Lasso): Adds the absolute values of the coefficients, encouraging sparse models where some features are eliminated.\n",
        "   - L2 regularization (Ridge): Adds the squared values of the coefficients, shrinking the weights without eliminating them.\n",
        "\n",
        "- Why is Regularization Needed? >>\n",
        "\n",
        "  - To Prevent Overfitting - Regularization prevents the model from fitting too closely to the training data, which helps avoid overfitting and ensures better performance on unseen data.\n",
        "\n",
        "  - To Improve Generalization - By penalizing large coefficients, regularization makes the model simpler and helps it generalize better to new data, improving accuracy on test sets.\n",
        "\n",
        "  - To Avoid Large Coefficients - Large coefficients can lead to overfitting, where the model is overly sensitive to small variations. Regularization helps control this by penalizing large weights.\n",
        "\n",
        "  - Feature Selection (with L1) - L1 regularization can automatically remove irrelevant features by driving their coefficients to zero, making the model more efficient and interpretable."
      ],
      "metadata": {
        "id": "t-bjTkW5dDCe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Difference between Lasso, Ridge, and Elastic Net Regression ?\n",
        "- These are all regularization techniques used to improve the performance of regression models by preventing overfitting and controlling model complexity.\n",
        "\n",
        "- Lasso Regression (L1 Regularization)\n",
        "  - Penalty: Adds the absolute values of the coefficients to the cost function.\n",
        "  - Effect: Can shrink some coefficients to exactly zero, effectively performing feature selection.\n",
        "  - Best for: When only a few features are important and the rest can be ignored.\n",
        "\n",
        "- Ridge Regression (L2 Regularization)\n",
        "  - Penalty: Adds the squared values of the coefficients to the cost function.\n",
        "  - Effect: Shrinks all coefficients closer to zero, but none become exactly zero.\n",
        "  - Best for: When many features contribute and all should be included with small weights.\n",
        "\n",
        "- Elastic Net Regression (Combination of L1 + L2)\n",
        "  - Penalty: Combines both L1 and L2 penalties.\n",
        "  - Effect: Balances between feature selection (like Lasso) and coefficient shrinkage (like Ridge).\n",
        "  - Best for: When there are many correlated features or when you want a mix of sparsity and regularization."
      ],
      "metadata": {
        "id": "Spzd21r9dDFA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. When Should We Use Elastic Net Instead of Lasso or Ridge?\n",
        "- Elastic Net is most useful when Lasso and Ridge alone don't perform well due to the nature of the data. It combines the strengths of both, making it ideal in the following situations:\n",
        "\n",
        "- When Features Are Highly Correlated\n",
        "  - If you have many correlated features, Lasso tends to randomly pick one and ignore the others.\n",
        "  - Elastic Net handles this better by grouping correlated variables and keeping them together.\n",
        "\n",
        "- When You Need Both Feature Selection and Regularization\n",
        "  - Lasso does feature selection, and Ridge does regularization.\n",
        "  - Elastic Net gives you sparsity (from Lasso) and stability (from Ridge), balancing the two.\n",
        "\n",
        "- When Lasso Is Too Aggressive\n",
        "  - Lasso might eliminate too many features, especially when features are correlated or weakly predictive.\n",
        "  - Elastic Net retains more features while still reducing complexity.\n",
        "\n",
        "- When Dataset Has More Features than Observations (p > n)\n",
        "  - In high-dimensional data (e.g., gene expression datasets), Lasso may struggle.\n",
        "  - Elastic Net performs better because it doesn't rely solely on L1 penalty, which may be unstable in such cases.\n",
        "\n"
      ],
      "metadata": {
        "id": "f9N27eGzdDHh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. What is the Impact of the Regularization Parameter (λ) in Logistic Regression?\n",
        "- The regularization parameter λ (lambda) controls the strength of the penalty applied to the logistic regression model. It plays a crucial role in balancing the trade-off between fitting the training data well and keeping the model simple to generalize better on unseen data.\n",
        "\n",
        "- When λ is Very Small (Close to 0):\n",
        "  - The model applies little to no regularization, meaning it tries to fit the training data as closely as possible.\n",
        "  - This can lead to overfitting, where the model performs well on the training set but poorly on the test set because it has learned noise or irrelevant patterns.\n",
        "\n",
        "- When λ is Very Large:\n",
        "  - The regularization term dominates the cost function, forcing the model to reduce the size of the coefficients drastically.\n",
        "  - This leads to underfitting, where the model is too simple and fails to capture the underlying patterns in the data\n",
        "  - The predictions become less accurate both on training and test data.\n",
        "\n",
        "- Optimal λ (Balanced Value):\n",
        "  - An optimal λ finds the right balance between underfitting and overfitting.\n",
        "  - It keeps the model simple yet effective, ensuring that it generalizes well to unseen data.\n",
        "  - This value is usually determined using techniques like cross-validation.\n",
        "\n",
        "- Effect on Feature Coefficients:\n",
        "  - As λ increases, the magnitude of the coefficients decreases.\n",
        "  - In Lasso (L1), large λ can drive some coefficients to exactly zero, performing feature selection.\n",
        "  - In Ridge (L2), coefficients are shrunk but not eliminated, maintaining all features but with reduced influence."
      ],
      "metadata": {
        "id": "Cee-wH3jsxih"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. What Are the Key Assumptions of Logistic Regression?\n",
        "- Logistic regression, like any statistical model, relies on a few core assumptions to ensure that its predictions and inferences are valid and reliable. These assumptions help maintain the integrity of the model and its outcomes.\n",
        "\n",
        "- Binary or Categorical Dependent Variable\n",
        "  - Logistic regression assumes that the target (dependent) variable is binary (e.g., 0 or 1, Yes or No).\n",
        "  - For multiclass problems, extensions like multinomial logistic regression are used.\n",
        "\n",
        "- Linear Relationship Between Features and Log-Odds\n",
        "  - It assumes a linear relationship between the independent variables and the log-odds of the dependent variable.\n",
        "  - Unlike linear regression, logistic regression does not assume a linear relationship with the actual outcome, but with the logit (log-odds) function.\n",
        "\n",
        "- No or Minimal Multicollinearity Among Features\n",
        "  - Features (independent variables) should not be highly correlated with each other.\n",
        "  - Multicollinearity can make it difficult to interpret coefficients and can destabilize the model.\n",
        "\n",
        "- Independence of Observations\n",
        "  - Each observation in the dataset should be independent of the others.\n",
        "  - Violation of this assumption (like in time series or grouped data) can lead to misleading results.\n",
        "\n",
        "- Large Sample Size\n",
        "  - Logistic regression performs better with larger datasets, especially when the dependent variable has imbalanced classes.\n",
        "  - A larger sample provides more stable and reliable coefficient estimates.\n",
        "\n",
        "- No Extreme Outliers in Predictors\n",
        "  - Although logistic regression is more robust to outliers than linear regression, extreme values in independent variables can still distort the results."
      ],
      "metadata": {
        "id": "QgHpjLZpsxv5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. What Are Some Alternatives to Logistic Regression for Classification Tasks?\n",
        "- Logistic regression is often the first choice for binary classification because of its simplicity and interpretability. However, it has limitations, especially with non-linear relationships, imbalanced data, or complex feature interactions. In such cases, several alternative classification models can outperform it in terms of accuracy and flexibility.\n",
        "\n",
        "- Decision Tree Classifier\n",
        "  - A tree-based model that splits the data using decision rules on feature values.\n",
        "  - It mimics human decision-making and is easy to interpret.\n",
        "  - However, decision trees are prone to overfitting, especially on noisy data, unless regularization (like max depth or pruning) is applied.\n",
        "\n",
        "- Random Forest Classifier\n",
        "  - An ensemble method that builds multiple decision trees on different subsets of data and combines their predictions.\n",
        "  - It reduces overfitting seen in single decision trees and handles large feature sets well.\n",
        "  - Random Forests are robust, accurate, and can also estimate feature importance.\n",
        "\n",
        "- Support Vector Machine (SVM)\n",
        "  - SVM finds the optimal hyperplane that separates different classes with the maximum margin.\n",
        "  - It performs well in high-dimensional spaces and with non-linear boundaries using kernel tricks (like RBF or polynomial).\n",
        "  - It can be computationally intensive on large datasets and is sensitive to parameter tuning.\n",
        "\n",
        "- K-Nearest Neighbors (KNN)\n",
        "  - A lazy learning algorithm that classifies a new point based on the majority class among its k nearest neighbors.\n",
        "  - It requires no training but can be slow during prediction, especially for large datasets.\n",
        "  - KNN is sensitive to the choice of ‘k’ and the scale of features (so normalization is important)\n",
        "\n",
        "- Naive Bayes Classifier\n",
        "  - Based on Bayes' Theorem and assumes that all features are independent, which rarely holds in practice.\n",
        "  - Despite the simplicity of this assumption, Naive Bayes performs well, especially in text classification (e.g., spam filtering, sentiment analysis).\n",
        "  - It is extremely fast, even on large datasets.\n",
        "\n",
        "- Gradient Boosting Algorithms (XGBoost, LightGBM, CatBoost)\n",
        "  - These are ensemble models that build a strong classifier by combining many weak learners (like shallow trees).\n",
        "  - They work by minimizing the errors of previous models in a sequential manner.\n",
        "  - These models often win machine learning competitions due to their accuracy and flexibility."
      ],
      "metadata": {
        "id": "chgU8g5jsx6f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. What Are Classification Evaluation Metrics?\n",
        "- Classification evaluation metrics are used to measure the performance of a classification model. These metrics help us understand how well our model is able to predict the correct classes, especially in the presence of imbalanced data or varying costs of misclassification.\n",
        "- Below are the most commonly used evaluation metrics in classification tasks:\n",
        "\n",
        "- Accuracy\n",
        "  - Accuracy is the ratio of correctly predicted observations to the total observations.\n",
        "  - Formula: Accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
        "  - It gives a general idea of how often the classifier is correct. However, it can be misleading when the classes are imbalanced.\n",
        "\n",
        "- Precision\n",
        "  - Precision measures how many of the predicted positive cases are actually positive.\n",
        "  - Formula: Precision = TP / (TP + FP)\n",
        "  - It is useful when the cost of a false positive is high (e.g., in spam detection or disease diagnosis).\n",
        "\n",
        "- Recall (Sensitivity or True Positive Rate)\n",
        "  - Recall indicates how many actual positive cases were correctly identified by the model.\n",
        "  - Formula: Recall = TP / (TP + FN)\n",
        "  - This is important in cases where missing a positive prediction is risky (like fraud detection).\n",
        "\n",
        "- F1 Score\n",
        " - F1 Score is the harmonic mean of Precision and Recall, giving a balanced measure between the two.\n",
        " - Formula: F1 Score = 2 * (Precision * Recall) / (Precision + Recall)\n",
        " - It is especially useful when you need to balance precision and recall in imbalanced datasets.\n",
        "\n",
        "- ROC-AUC Score (Receiver Operating Characteristic - Area Under Curve)\n",
        "  - AUC measures the model's ability to distinguish between classes.\n",
        "  - It plots the True Positive Rate against the False Positive Rate at various thresholds.\n",
        "  - A value close to 1 means excellent model performance, while 0.5 means the model is guessing randomly.\n",
        "\n",
        "- Confusion Matrix\n",
        "  - A confusion matrix is a 2x2 table showing the counts of:\n",
        "TP (True Positives), FP (False Positives),\n",
        "TN (True Negatives), FN (False Negatives)\n",
        "  - It helps visualize where the model is making correct and incorrect predictions.\n",
        "\n",
        "- Specificity (True Negative Rate)\n",
        "  - Specificity measures how well the model identifies actual negative cases.\n",
        "  - Formula: Specificity = TN / (TN + FP)\n",
        "  - It is useful when it’s important not to falsely classify negatives as positives, such as avoiding false alarms in fraud alerts."
      ],
      "metadata": {
        "id": "nQ-oorK5syDe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "12. How Does Class Imbalance Affect Logistic Regression?\n",
        "- Class imbalance occurs when one class significantly outnumbers the other in a classification dataset. For example, in a fraud detection dataset, 98% of transactions might be non-fraud and only 2% fraud. This imbalance can negatively impact logistic regression and other classifiers in several ways:\n",
        "\n",
        "- Biased Predictions Toward Majority Class\n",
        "  - Logistic regression tries to minimize error across the dataset.\n",
        "  - In imbalanced data, the model may learn to predict the majority class more often just to increase overall accuracy.\n",
        "  - This leads to high accuracy but poor recall for the minority class, which is often the more important one (e.g., fraud cases).\n",
        "\n",
        "- Misleading Accuracy\n",
        "  - With imbalanced data, a model might achieve 95% or more accuracy simply by predicting the majority class every time.\n",
        "  - However, it may fail to correctly identify the minority class entirely.\n",
        "  - So, accuracy becomes a misleading metric in imbalanced classification problems.\n",
        "\n",
        "- Poor Recall or Sensitivity for Minority Class\n",
        "  - Logistic regression may fail to detect positive (minority) cases, resulting in high false negatives.\n",
        "  - In critical applications like disease diagnosis or loan default prediction, this can have serious consequences.\n",
        "\n",
        "- Less Reliable Decision Boundaries\n",
        "  - Logistic regression assumes a linear relationship and tries to fit a boundary between classes.\n",
        "  - With imbalance, the model might shift the decision boundary toward the minority class to reduce error, making it unreliable.\n",
        "\n",
        "- Poor Probability Calibration\n",
        "  - Logistic regression predicts probabilities. With class imbalance, predicted probabilities may become skewed, leading to poor thresholding and decision making.\n",
        "\n"
      ],
      "metadata": {
        "id": "uAYidy3KsyLt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "13. What Is Hyperparameter Tuning in Logistic Regression?\n",
        "- Hyperparameter tuning in logistic regression refers to the process of selecting the best set of hyperparameters that control how the model learns from data. These are not learned during training — they are set before the training process begins and have a direct impact on the model’s performance.\n",
        "\n",
        "- Key Hyperparameters in Logistic Regression:\n",
        "  - Penalty (Regularization Type)\n",
        "    - Controls the type of regularization used to prevent overfitting.\n",
        "    - Common options: 'l1' (Lasso), 'l2' (Ridge), or 'elasticnet' (Elastic Net).\n",
        "    - Choosing the right penalty type can improve model generalization.\n",
        "\n",
        "  - C (Inverse of Regularization Strength)\n",
        "    - It is the inverse of the regularization parameter (lambda).\n",
        "    - A lower value of C means stronger regularization (more penalty), helping prevent overfitting.\n",
        "    - A higher value of C allows the model to fit the training data more closely, which may lead to overfitting.\n",
        "\n",
        "  - Solver\n",
        "    - The optimization algorithm used for training the model.\n",
        "    - Options include 'liblinear', 'saga', 'lbfgs', etc.\n",
        "    - Some solvers work better with certain penalties (e.g., 'liblinear' supports both 'l1' and 'l2').\n",
        "\n",
        "  - Max_iter (Maximum Iterations)\n",
        "    - Sets the maximum number of iterations allowed for the solver to converge.\n",
        "    - Increasing this can help when the model fails to converge with the default value.\n",
        "\n",
        "- Proper tuning helps you find the best-performing model by balancing bias and variance.\n",
        "- It ensures the model is neither underfitting nor overfitting.\n",
        "- It leads to better generalization on unseen data."
      ],
      "metadata": {
        "id": "eAP4EtRusyTH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "14. What Are Different Solvers in Logistic Regression? Which One Should Be Used?\n",
        "- In logistic regression, solvers are algorithms used to optimize the cost function and find the best model parameters (weights). Each solver approaches this optimization problem differently based on mathematical methods such as gradient descent or Newton's method. The choice of solver can significantly impact the training speed, model accuracy, and type of regularization supported.\n",
        "\n",
        "- liblinear\n",
        "  - liblinear is based on the coordinate descent algorithm and is used by default in some implementations like scikit-learn (for binary classification).\n",
        "  - It works well on small datasets and provides good performance for models with relatively few features\n",
        "  - Supports both L1 and L2 regularization, which means it can be used for both Lasso and Ridge logistic regression.\n",
        "  - However, it does not support multinomial loss, so it’s limited to binary classification tasks.\n",
        "\n",
        "- saga\n",
        "  - saga is a more recent and powerful solver based on stochastic gradient descent.\n",
        "  - It is designed to handle large datasets and works well with sparse data (e.g., text classification using bag-of-words).\n",
        "  - Supports L1, L2, and Elastic Net regularization, making it extremely flexible.\n",
        "  - It can also handle both binary and multiclass classification, making it more versatile.\n",
        "\n",
        "-  lbfgs (Limited-memory BFGS)\n",
        "  - lbfgs is a quasi-Newton optimization algorithm. It’s fast and efficient for moderate to large datasets.\n",
        "  - It is widely used because of its ability to handle multinomial logistic regression, which allows it to work well for multiclass classification problems.\n",
        "  - It only supports L2 regularization, and may not work well with sparse data as efficiently as saga.\n",
        "\n",
        "- newton-cg\n",
        "  - newton-cg is an optimization algorithm that uses second-order derivative information (Hessian matrix).\n",
        "  - This can lead to more accurate solutions but at the cost of higher computational resources.\n",
        "  - Like lbfgs, it supports only L2 regularization and works for multiclass problems.\n",
        "\n",
        "- sag (Stochastic Average Gradient)\n",
        "  - sag is a variant of stochastic gradient descent that works well for very large datasets with many samples.\n",
        "  - It is particularly efficient when the number of training samples is in the hundreds of thousands or millions.\n",
        "  - Only supports L2 regularization, so it’s not suitable if you require L1 or Elastic Net.\n",
        "\n"
      ],
      "metadata": {
        "id": "G36VAP7GsyZ7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "15. How Is Logistic Regression Extended for Multiclass Classification?\n",
        "- Logistic regression is originally a binary classification algorithm, meaning it is designed to classify data into one of two categories. However, in real-world scenarios, we often encounter multiclass classification problems, where the number of classes is greater than two. To extend logistic regression for such tasks, two main strategies are commonly used: One-vs-Rest (OvR) and Multinomial Logistic Regression (also known as Softmax Regression).\n",
        "\n",
        "- In the One-vs-Rest approach, the model splits the multiclass problem into multiple binary classification problems. For each class, it builds a separate classifier that distinguishes that class from all the others. For example, in a three-class problem (say A, B, and C), three logistic regression models will be trained: one for A vs not A, one for B vs not B, and one for C vs not C. When making predictions, the model evaluates all classifiers and selects the class with the highest confidence score. This method is simple and easy to implement, and works well when the number of classes is small, but it may suffer from performance issues if the classes overlap significantly.\n",
        "\n",
        "- On the other hand, Multinomial Logistic Regression (Softmax Regression) treats the problem as a single multiclass classification task rather than breaking it down into binary subproblems. It uses the softmax function to calculate the probability of the input belonging to each class and then selects the class with the highest probability. This method is generally more accurate and efficient in scenarios with balanced class distributions and when class relationships matter. However, it is computationally heavier and requires specific solvers like 'lbfgs', 'saga', or 'newton-cg' to work properly.\n",
        "\n",
        "- In practice, OvR is easier to interpret and often used when interpretability is important, while multinomial logistic regression is preferred for better probability estimation and overall accuracy in multiclass settings."
      ],
      "metadata": {
        "id": "YuZdTSQasydt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "16. What Are the Advantages and Disadvantages of Logistic Regression?\n",
        "- Advantages of Logistic RegressioN >>\n",
        "\n",
        "- Simple and Easy to Implement - Logistic regression is straightforward to understand and quick to implement, making it ideal for beginners and baseline models.\n",
        "\n",
        "- Interpretable Results - The model provides coefficients that show the relationship between features and the outcome, helping with decision-making in fields like healthcare or finance.\n",
        "\n",
        "- Fast Training Time - It requires low computational power and works efficiently with smaller datasets.\n",
        "\n",
        "- Works Well with Linearly Separable Data - It performs well when the classes can be separated with a straight line (or hyperplane in higher dimensions).\n",
        "\n",
        "- Probabilistic Output - It predicts probabilities for each class, which is useful in many practical applications such as credit scoring.\n",
        "\n",
        "- Good Baseline Model - It is often used as a first model to compare against more complex models.\n",
        "\n",
        "- Disadvantages of Logistic Regression >>\n",
        "\n",
        "- Assumes Linear Relationship - It assumes a linear relationship between the independent variables and the log-odds, which may not always be true in real-world data.\n",
        "\n",
        "- Not Suitable for Complex Relationships - Logistic regression cannot capture non-linear patterns unless non-linear transformations or feature engineering are applied.\n",
        "\n",
        "- Sensitive to Outliers - Outliers can skew the model significantly because it uses a linear decision boundary.\n",
        "\n",
        "- Poor Performance on High-Dimensional or Noisy Data - It may overfit or underperform when dealing with too many irrelevant features or noisy data.\n",
        "\n",
        "- Needs Feature Scaling and Cleaning - It often requires preprocessing like removing multicollinearity, scaling variables, and handling missing values for optimal performance.\n",
        "\n",
        "- Struggles with Imbalanced Data - Logistic regression may be biased toward the majority class unless balancing techniques like SMOTE or class weights are applied.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "CIFnIszesypn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "17. What Are Some Use Cases of Logistic Regression?\n",
        "- Common Use Cases of Logistic Regression:\n",
        "\n",
        "- Disease Prediction (Healthcare)\n",
        "  - Logistic Regression is heavily used to predict the probability of a patient having a specific disease such as diabetes, cancer, or heart disease. By using input features like age, BMI, blood pressure, cholesterol levels, and lifestyle habits, doctors and medical systems can classify patients as high or low risk. It supports early diagnosis and treatment planning by providing interpretable results.\n",
        "\n",
        "- Customer Churn Prediction (Marketing/SaaS)\n",
        "  - Companies use logistic regression to estimate the likelihood of a customer canceling a subscription or stopping the use of a product. This is based on past behaviors such as frequency of use, support interactions, complaints, or recent changes in activity. Churn models help businesses retain customers by targeting at-risk users with offers or engagement strategies.\n",
        "\n",
        "- Credit Scoring (Finance/Banking)\n",
        "  - Banks and credit card companies use logistic regression to determine if a loan applicant should be approved or rejected. It looks at factors such as income, credit history, number of defaults, and employment status to predict whether a person will repay the loan or default. It’s a standard tool for risk assessment in lending.\n",
        "\n",
        "- Fraud Detection (Finance/Security)\n",
        "  - Financial institutions employ logistic regression to detect fraudulent transactions by learning patterns in historical data. By analyzing variables like transaction amount, location, time, and frequency, the model can flag suspicious activities. It helps reduce financial losses and enhances security by triggering alerts in real-time.\n",
        "\n",
        "- Email Spam Detection (IT/Security)\n",
        "  - Logistic regression is used in spam filters to classify emails as spam or legitimate. It relies on features such as the presence of specific keywords, sender reputation, and email structure. It is one of the core techniques used in email clients to clean up inboxes automatically and prevent phishing.\n",
        "\n",
        "- Lead Conversion Prediction (Sales)\n",
        "  - Sales teams use logistic regression to identify high-potential leads that are most likely to convert into paying customers. Features such as engagement with emails, number of visits to the website, time spent on product pages, and demographics help prioritize leads and improve conversion rates.\n",
        "\n",
        "- Employee Attrition Prediction (HR Analytics)\n",
        "  - HR departments apply logistic regression to assess which employees are likely to leave the company. The model uses data like years at the company, job satisfaction, salary, and promotion history. Predicting attrition helps in planning retention strategies and improving employee satisfaction.\n",
        "\n",
        "- Political Voting Prediction (Social Science)\n",
        "  - In surveys and political analysis, logistic regression predicts whether a person is likely to vote and which political party they may support. Factors include age, income, education, geographic location, and historical voting behavior. It is commonly used in pre-election studies and exit polls."
      ],
      "metadata": {
        "id": "YDOn2CJTMqW7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "18. What is the difference between Softmax Regression and Logistic Regression?\n",
        "- Differences Between Softmax Regression and Logistic Regression:\n",
        "\n",
        "- Type of Classification\n",
        "  - Softmax Regression is used for multiclass classification (i.e., three or more classes).\n",
        "  - Logistic Regression is used for binary classification (i.e., two classes: 0 or 1).\n",
        "\n",
        "- Function Used\n",
        "  - Softmax Regression uses the Softmax function to output a probability distribution over multiple classes.\n",
        "  - Logistic Regression uses the Sigmoid function to map outputs between 0 and 1.\n",
        "\n",
        "- Output\n",
        "  - Softmax regression returns probabilities for all classes, and all those probabilities add up to 1.\n",
        "  - Logistic regression returns the probability of one class (typically class 1); the other is just 1 minus that.\n",
        "\n",
        "- Interpretation of Results\n",
        "  - Softmax regression tells how likely an instance belongs to each class out of multiple.\n",
        "  - Logistic regression tells how likely an instance belongs to one class.\n",
        "\n",
        "- Use Cases\n",
        "  - Softmax regression is used in image classification, sentiment analysis, etc., where there are multiple categories.\n",
        "  - Logistic regression is used in tasks like spam detection, loan approval (yes/no).\n",
        "\n",
        "- Decision Boundary\n",
        "  - Softmax regression has multiple boundaries, each separating one class from the rest.\n",
        "  - Logistic regression has a single linear decision boundary between two classes.\n",
        "\n",
        "- Computational Complexity\n",
        "  - Softmax regression is more computationally expensive due to handling multiple outputs.\n",
        "  - Logistic regression is simpler and faster to compute."
      ],
      "metadata": {
        "id": "wYb6V7zaMqij"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "19. How do we choose between One-vs-Rest (OvR) and Softmax for multiclass classification?\n",
        "- Difference Between One-vs-Rest (OvR) and Softmax Approach:\n",
        "\n",
        "- Basic Concept\n",
        "  - OvR (One-vs-Rest) trains one binary classifier per class, treating it as “that class” vs “all others”.\n",
        "  - Softmax trains a single model that directly predicts the probability distribution across all classes.\n",
        "\n",
        "- Model Architecture\n",
        "  - OvR uses multiple binary logistic regression models, one for each class.\n",
        "  - Softmax uses one multiclass logistic regression model with multiple outputs.\n",
        "\n",
        "- Prediction Mechanism\n",
        "  - In OvR, each model gives a score, and the class with the highest score is selected.\n",
        "  - In Softmax, the class with the highest probability (from the softmax function) is selected.\n",
        "\n",
        "- Training Efficiency\n",
        "  - OvR can be faster to train when the number of classes is small.\n",
        "  - Softmax can be more efficient when using vectorized implementations in deep learning frameworks.\n",
        "\n",
        "- Interpretability\n",
        "  - OvR provides more transparent decision logic per class, useful for model debugging.\n",
        "  - Softmax provides a probability distribution across all classes in a single shot, making it elegant but less interpretable.\n",
        "\n",
        "- Use Case Suitability\n",
        "  - Use OvR when:\n",
        "    - You want to keep binary classification models.\n",
        "    - The number of classes is large and sparse.\n",
        "    - You need separate control over each class.\n",
        "\n",
        "  - Use Softmax when:\n",
        "    - You need a single unified model for multiclass prediction.\n",
        "    - You’re working in deep learning or frameworks like TensorFlow/Keras.\n",
        "    - The classes are mutually exclusive and require probabilistic output.\n",
        "\n",
        "- Model Performance\n",
        "  - OvR can perform better if some classes are highly imbalanced or rare.\n",
        "  - Softmax is generally preferred for balanced and dense multiclass datasets.\n",
        "\n",
        "- Choose OvR for simple, interpretable models, especially when working with classical ML algorithms like SVM or logistic regression manually.\n",
        "- Choose Softmax when using neural networks or when you want a compact and probabilistic output over multiple classes."
      ],
      "metadata": {
        "id": "WWsHCI-TMqsk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "20. How do we interpret coefficients in Logistic Regression?\n",
        "-  Interpretation of Coefficients in Logistic Regression:\n",
        "\n",
        "- Log-Odds Interpretation\n",
        "  - In Logistic Regression, the coefficients represent the log-odds of the outcome (e.g., success or failure) relative to the predictor variables.\n",
        "  - For a coefficient, it means that for each one-unit increase in the predictor variable, the log-odds of the outcome change by that coefficient value, holding all other variables constant.\n",
        "\n",
        "- Odds Ratio (Exponential of Coefficients)\n",
        "  - To better interpret the coefficients, we often take the exponent of the coefficient (e^coefficient), which converts log-odds to odds ratio.\n",
        "  - The odds ratio tells us how the odds of the outcome increase or decrease when the predictor variable increases by one unit.\n",
        "  - Example: If the coefficient is 0.5, then e^0.5 ≈ 1.65, meaning that for every one-unit increase in the predictor variable, the odds of the outcome happening increase by a factor of 1.65.\n",
        "\n",
        "- Positive Coefficients\n",
        "  - A positive coefficient means that as the predictor variable increases, the odds of the outcome occurring increase\n",
        "  - For instance, if the coefficient of a predictor variable like age is positive, it suggests that as age increases, the probability of the outcome (e.g., having a disease) increases.\n",
        "\n",
        "- Negative Coefficients\n",
        "  - A negative coefficient means that as the predictor variable increases, the odds of the outcome decrease.\n",
        "  - For example, if the coefficient of a variable like \"hours of exercise per week\" is negative, it indicates that more exercise decreases the likelihood of a disease occurring.\n",
        "\n",
        "- Magnitude of Coefficients\n",
        "  - The magnitude of the coefficient indicates the strength of the relationship between the predictor and the outcome.\n",
        "  - Larger absolute values of coefficients indicate stronger relationships with the outcome variable, meaning that small changes in the predictor can have a large effect on the outcome.\n",
        "\n",
        "- Standardized Coefficients\n",
        "  - Standardizing the variables before fitting the model allows you to compare the relative importance of different predictors.\n",
        "  - Larger standardized coefficients indicate more significant predictors in the model, while smaller ones suggest less importance.\n",
        "\n",
        "- Multicollinearity Consideration\n",
        "  - When predictors are highly correlated with each other (multicollinearity), the coefficients may become less reliable, and interpretation can become challenging.\n",
        "  - It’s essential to check for multicollinearity (using VIFs or correlation matrices) to ensure valid interpretations of the coefficients.\n",
        "\n",
        "- Intercept (Bias Term)\n",
        "  - The intercept term in Logistic Regression represents the log-odds of the outcome when all predictor variables are zero.\n",
        "  - It is often not directly interpretable, but it sets the baseline for the predicted probability."
      ],
      "metadata": {
        "id": "MfXIOaFFMq2O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#PRACTICAL QUESTIONS"
      ],
      "metadata": {
        "id": "m9SARPnHMq_p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#1. Write a Python program that loads a dataset, splits it into training and testing sets, applies Logistic Regression, and prints the model accuracy.\n",
        "\n",
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the dataset (use the correct path to your CSV file)\n",
        "dataset = pd.read_csv('/content/customer_data.csv')\n",
        "\n",
        "# Check the first few rows of the dataset\n",
        "print(dataset.head())\n",
        "\n",
        "# Define the feature columns (X) and target column (y)\n",
        "X = dataset[['Age', 'Salary']]  # Use actual feature column names\n",
        "y = dataset['Purchased']  # Use actual target column name\n",
        "\n",
        "# Split the data into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the Logistic Regression model\n",
        "model = LogisticRegression()\n",
        "\n",
        "# Train the model on the training data\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy of the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print the model accuracy\n",
        "print(f'Model Accuracy: {accuracy * 100:.2f}%')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zw2ukcx4TKlq",
        "outputId": "a8df6d6c-9bb8-4a07-971e-0f79aefcaf4e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Age  Salary  Purchased\n",
            "0   22   15000          0\n",
            "1   25   25000          0\n",
            "2   27   30000          1\n",
            "3   30   35000          1\n",
            "4   35   40000          1\n",
            "Model Accuracy: 0.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#2. Write a Python program to apply L1 regularization (Lasso) on a dataset using Logistic Regression (penalty='l1') and print the model accuracy.\n",
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the dataset (use the correct path to your CSV file)\n",
        "dataset = pd.read_csv('customer_data.csv')\n",
        "\n",
        "# Check the first few rows of the dataset\n",
        "print(dataset.head())\n",
        "\n",
        "# Define the feature columns (X) and target column (y)\n",
        "X = dataset[['Age', 'Salary']]  # Feature columns\n",
        "y = dataset['Purchased']  # Target column\n",
        "\n",
        "# Split the data into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the Logistic Regression model with L1 regularization (Lasso)\n",
        "model = LogisticRegression(penalty='l1', solver='liblinear')  # 'liblinear' solver supports L1 penalty\n",
        "\n",
        "# Train the model on the training data\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy of the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print the model accuracy\n",
        "print(f'Model Accuracy with L1 Regularization (Lasso): {accuracy * 100:.2f}%')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bD0yEZLBTKo5",
        "outputId": "80fe37f6-713c-450c-ed74-0d7d95a66a12"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Age  Salary  Purchased\n",
            "0   22   15000          0\n",
            "1   25   25000          0\n",
            "2   27   30000          1\n",
            "3   30   35000          1\n",
            "4   35   40000          1\n",
            "Model Accuracy with L1 Regularization (Lasso): 100.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/svm/_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#3. Write a Python program to train Logistic Regression with L2 regularization (Ridge) using Logistic Regression (penalty='l2'). Print model accuracy and coefficients.\n",
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the dataset (use the correct path to your CSV file)\n",
        "dataset = pd.read_csv('customer_data.csv')\n",
        "\n",
        "# Check the first few rows of the dataset\n",
        "print(dataset.head())\n",
        "\n",
        "# Define the feature columns (X) and target column (y)\n",
        "X = dataset[['Age', 'Salary']]  # Feature columns\n",
        "y = dataset['Purchased']  # Target column\n",
        "\n",
        "# Split the data into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the Logistic Regression model with L2 regularization (Ridge)\n",
        "model = LogisticRegression(penalty='l2', solver='liblinear')  # 'liblinear' solver supports L2 penalty\n",
        "\n",
        "# Train the model on the training data\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy of the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print the model accuracy\n",
        "print(f'Model Accuracy with L2 Regularization (Ridge): {accuracy * 100:.2f}%')\n",
        "\n",
        "# Print the model coefficients (weights)\n",
        "print(f'Model Coefficients: {model.coef_}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_akjyFxjTKru",
        "outputId": "dd815f79-45fe-4807-e1e7-cafbfea1d907"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Age  Salary  Purchased\n",
            "0   22   15000          0\n",
            "1   25   25000          0\n",
            "2   27   30000          1\n",
            "3   30   35000          1\n",
            "4   35   40000          1\n",
            "Model Accuracy with L2 Regularization (Ridge): 100.00%\n",
            "Model Coefficients: [[-4.16142890e-01  3.89165603e-04]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#4. Write a Python program to train Logistic Regression with ElasticNet regularization (penalty='elasticnet').\n",
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the dataset (use the correct path to your CSV file)\n",
        "dataset = pd.read_csv('customer_data.csv')\n",
        "\n",
        "# Check the first few rows of the dataset\n",
        "print(dataset.head())\n",
        "\n",
        "# Define the feature columns (X) and target column (y)\n",
        "X = dataset[['Age', 'Salary']]  # Feature columns\n",
        "y = dataset['Purchased']  # Target column\n",
        "\n",
        "# Split the data into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the Logistic Regression model with ElasticNet regularization\n",
        "model = LogisticRegression(penalty='elasticnet', solver='saga', l1_ratio=0.5)  # l1_ratio: 0.5 means equal L1 and L2 regularization\n",
        "\n",
        "# Train the model on the training data\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy of the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print the model accuracy\n",
        "print(f'Model Accuracy with ElasticNet Regularization: {accuracy * 100:.2f}%')\n",
        "\n",
        "# Print the model coefficients (weights)\n",
        "print(f'Model Coefficients: {model.coef_}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5NAPwZ-tTKu0",
        "outputId": "cbf040e2-c726-414a-b798-4a541d0c9607"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Age  Salary  Purchased\n",
            "0   22   15000          0\n",
            "1   25   25000          0\n",
            "2   27   30000          1\n",
            "3   30   35000          1\n",
            "4   35   40000          1\n",
            "Model Accuracy with ElasticNet Regularization: 50.00%\n",
            "Model Coefficients: [[-2.35460022e-07  1.24997639e-05]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#5. Write a Python program to train a Logistic Regression model for multiclass classification using multi_class='ovr'.\n",
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Sample dataset: Iris dataset (built-in from sklearn)\n",
        "from sklearn.datasets import load_iris\n",
        "iris = load_iris()\n",
        "\n",
        "# Load features and target\n",
        "X = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
        "y = pd.Series(iris.target)\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize Logistic Regression with one-vs-rest strategy\n",
        "model = LogisticRegression(multi_class='ovr', solver='liblinear')  # 'liblinear' is preferred for OvR\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict the classes on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print accuracy\n",
        "print(f'Model Accuracy using One-vs-Rest: {accuracy * 100:.2f}%')\n",
        "\n",
        "# Print model coefficients\n",
        "print('Model Coefficients:')\n",
        "print(model.coef_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XegbPNeITKzL",
        "outputId": "74439627-4dd5-4cb7-d955-add5834c5885"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy using One-vs-Rest: 100.00%\n",
            "Model Coefficients:\n",
            "[[ 0.3711229   1.409712   -2.15210117 -0.95474179]\n",
            " [ 0.49400451 -1.58897112  0.43717015 -1.11187838]\n",
            " [-1.55895271 -1.58893375  2.39874554  2.15556209]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#6. Write a Python program to apply GridSearchCV to tune the hyperparameters (C and penalty) of Logistic Regression. Print the best parameters and accuracy.\n",
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset (multiclass classification)\n",
        "iris = load_iris()\n",
        "X = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
        "y = pd.Series(iris.target)\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the model\n",
        "log_reg = LogisticRegression(solver='liblinear', multi_class='ovr')\n",
        "\n",
        "# Define the parameter grid to tune\n",
        "param_grid = {\n",
        "    'C': [0.01, 0.1, 1, 10, 100],       # Regularization strengths\n",
        "    'penalty': ['l1', 'l2']             # Type of regularization\n",
        "}\n",
        "\n",
        "# Apply GridSearchCV\n",
        "grid_search = GridSearchCV(log_reg, param_grid, cv=5)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best parameters\n",
        "best_params = grid_search.best_params_\n",
        "print(f'Best Parameters: {best_params}')\n",
        "\n",
        "# Predict on test set using best estimator\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "# Calculate and print accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Accuracy with Best Parameters: {accuracy * 100:.2f}%')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tg9H0O62TK1s",
        "outputId": "5676ef62-2bea-45aa-9fa2-a26da28267f6"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'C': 10, 'penalty': 'l1'}\n",
            "Accuracy with Best Parameters: 100.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/svm/_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#7. Write a Python program to evaluate Logistic Regression using Stratified K-Fold Cross-Validation. Print the average accuracy.\n",
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
        "y = pd.Series(iris.target)\n",
        "\n",
        "# Define the Logistic Regression model\n",
        "model = LogisticRegression(solver='liblinear', multi_class='ovr')\n",
        "\n",
        "# Initialize Stratified K-Fold Cross Validator\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Perform cross-validation and compute accuracy scores\n",
        "scores = cross_val_score(model, X, y, cv=skf, scoring='accuracy')\n",
        "\n",
        "# Print accuracy for each fold\n",
        "print(\"Accuracy for each fold:\", scores)\n",
        "\n",
        "# Print average accuracy\n",
        "print(f\"Average Accuracy: {scores.mean() * 100:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "stfRDxpjTK4O",
        "outputId": "cbd7ea4c-f59a-4a34-bde9-cd6c8f5e9c75"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy for each fold: [0.96666667 1.         0.9        0.93333333 1.        ]\n",
            "Average Accuracy: 96.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#8. Write a Python program to load a dataset from a CSV file, apply Logistic Regression, and evaluate its accuracy.\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from scipy.stats import uniform\n",
        "\n",
        "# Load the larger dataset\n",
        "dataset = pd.read_csv('/content/large_logistic_data.csv')\n",
        "\n",
        "X = dataset[['age', 'salary', 'education_level']]\n",
        "y = dataset['purchased']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Set up logistic regression model\n",
        "logreg = LogisticRegression(max_iter=1000)\n",
        "\n",
        "# Define the hyperparameter space\n",
        "param_distributions = {\n",
        "    'C': uniform(loc=0.01, scale=10),  # Regularization parameter C\n",
        "    'penalty': ['l1', 'l2', 'elasticnet', 'none'],  # Regularization types\n",
        "    'solver': ['saga', 'liblinear', 'lbfgs', 'newton-cg', 'sag']  # Solvers available for Logistic Regression\n",
        "}\n",
        "\n",
        "# Use StratifiedKFold for cross-validation to handle class imbalance\n",
        "cv_strategy = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Perform RandomizedSearchCV\n",
        "random_search = RandomizedSearchCV(\n",
        "    estimator=logreg,\n",
        "    param_distributions=param_distributions,\n",
        "    n_iter=20,  # Number of iterations\n",
        "    cv=cv_strategy,\n",
        "    scoring='accuracy',\n",
        "    random_state=42,\n",
        "    n_jobs=-1  # Use all cores\n",
        ")\n",
        "\n",
        "# Fit the model to the training data\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best model and evaluate it on the test set\n",
        "best_model = random_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print the best hyperparameters and the accuracy\n",
        "print(\"Best Hyperparameters:\", random_search.best_params_)\n",
        "print(f\"Accuracy with Best Parameters: {accuracy * 100:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tPBjd6Y7TK6t",
        "outputId": "a45b5089-4f94-4caa-e262-02bbe8180c6b"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Hyperparameters: {'C': np.float64(7.806910002727692), 'penalty': 'l1', 'solver': 'liblinear'}\n",
            "Accuracy with Best Parameters: 91.50%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py:528: FitFailedWarning: \n",
            "60 fits failed out of a total of 100.\n",
            "The score on these train-test partitions for these parameters will be set to nan.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "10 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/base.py\", line 1389, in wrapper\n",
            "    return fit_method(estimator, *args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py\", line 1193, in fit\n",
            "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py\", line 63, in _check_solver\n",
            "    raise ValueError(\n",
            "ValueError: Solver lbfgs supports only 'l2' or None penalties, got l1 penalty.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "5 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/base.py\", line 1389, in wrapper\n",
            "    return fit_method(estimator, *args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py\", line 1193, in fit\n",
            "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py\", line 63, in _check_solver\n",
            "    raise ValueError(\n",
            "ValueError: Solver lbfgs supports only 'l2' or None penalties, got elasticnet penalty.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "35 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/base.py\", line 1382, in wrapper\n",
            "    estimator._validate_params()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/base.py\", line 436, in _validate_params\n",
            "    validate_parameter_constraints(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/utils/_param_validation.py\", line 98, in validate_parameter_constraints\n",
            "    raise InvalidParameterError(\n",
            "sklearn.utils._param_validation.InvalidParameterError: The 'penalty' parameter of LogisticRegression must be a str among {'elasticnet', 'l1', 'l2'} or None. Got 'none' instead.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "5 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/base.py\", line 1389, in wrapper\n",
            "    return fit_method(estimator, *args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py\", line 1193, in fit\n",
            "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py\", line 71, in _check_solver\n",
            "    raise ValueError(\n",
            "ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "5 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/base.py\", line 1389, in wrapper\n",
            "    return fit_method(estimator, *args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py\", line 1193, in fit\n",
            "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py\", line 63, in _check_solver\n",
            "    raise ValueError(\n",
            "ValueError: Solver sag supports only 'l2' or None penalties, got elasticnet penalty.\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_search.py:1108: UserWarning: One or more of the test scores are non-finite: [    nan 0.88625     nan     nan 0.885   0.885       nan     nan     nan\n",
            "     nan     nan     nan 0.885   0.88375 0.88375     nan     nan     nan\n",
            " 0.88375 0.885  ]\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#9. Write a Python program to apply RandomizedSearchCV for tuning hyperparameters (C, penalty, solver) in Logistic Regression. Print the best parameters and accuracy.\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load the dataset\n",
        "dataset = pd.read_csv(\"large_logistic_data.csv\")\n",
        "\n",
        "# Define features (X) and target (y)\n",
        "X = dataset[['age', 'salary', 'education_level']]\n",
        "y = dataset['purchased']\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Feature scaling\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Define logistic regression model\n",
        "logreg = LogisticRegression(max_iter=1000)\n",
        "\n",
        "# Hyperparameter grid\n",
        "param_distributions = {\n",
        "    'C': [0.01, 0.1, 1, 10, 100],\n",
        "    'penalty': ['l1', 'l2', 'elasticnet', 'none'],\n",
        "    'solver': ['saga', 'liblinear', 'lbfgs', 'newton-cg', 'sag']\n",
        "}\n",
        "\n",
        "# Run RandomizedSearchCV\n",
        "random_search = RandomizedSearchCV(\n",
        "    estimator=logreg,\n",
        "    param_distributions=param_distributions,\n",
        "    n_iter=20,\n",
        "    cv=3,\n",
        "    scoring='accuracy',\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# Fit and evaluate\n",
        "random_search.fit(X_train, y_train)\n",
        "best_model = random_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "# Print results\n",
        "print(\"Best Parameters:\", random_search.best_params_)\n",
        "print(\"Model Accuracy:\", accuracy_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OAjlp7vqTK8n",
        "outputId": "eafe4d96-31e6-4903-fde3-eacf4a49f35a"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'solver': 'saga', 'penalty': 'l1', 'C': 0.01}\n",
            "Model Accuracy: 0.89\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py:528: FitFailedWarning: \n",
            "51 fits failed out of a total of 60.\n",
            "The score on these train-test partitions for these parameters will be set to nan.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "3 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/base.py\", line 1389, in wrapper\n",
            "    return fit_method(estimator, *args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py\", line 1193, in fit\n",
            "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py\", line 63, in _check_solver\n",
            "    raise ValueError(\n",
            "ValueError: Solver newton-cg supports only 'l2' or None penalties, got l1 penalty.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "9 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/base.py\", line 1389, in wrapper\n",
            "    return fit_method(estimator, *args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py\", line 1193, in fit\n",
            "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py\", line 63, in _check_solver\n",
            "    raise ValueError(\n",
            "ValueError: Solver newton-cg supports only 'l2' or None penalties, got elasticnet penalty.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "12 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/base.py\", line 1389, in wrapper\n",
            "    return fit_method(estimator, *args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py\", line 1203, in fit\n",
            "    raise ValueError(\"l1_ratio must be specified when penalty is elasticnet.\")\n",
            "ValueError: l1_ratio must be specified when penalty is elasticnet.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "6 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/base.py\", line 1389, in wrapper\n",
            "    return fit_method(estimator, *args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py\", line 1193, in fit\n",
            "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py\", line 63, in _check_solver\n",
            "    raise ValueError(\n",
            "ValueError: Solver sag supports only 'l2' or None penalties, got l1 penalty.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "12 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/base.py\", line 1382, in wrapper\n",
            "    estimator._validate_params()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/base.py\", line 436, in _validate_params\n",
            "    validate_parameter_constraints(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/utils/_param_validation.py\", line 98, in validate_parameter_constraints\n",
            "    raise InvalidParameterError(\n",
            "sklearn.utils._param_validation.InvalidParameterError: The 'penalty' parameter of LogisticRegression must be a str among {'l2', 'l1', 'elasticnet'} or None. Got 'none' instead.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "3 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/base.py\", line 1389, in wrapper\n",
            "    return fit_method(estimator, *args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py\", line 1193, in fit\n",
            "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py\", line 63, in _check_solver\n",
            "    raise ValueError(\n",
            "ValueError: Solver lbfgs supports only 'l2' or None penalties, got l1 penalty.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "3 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/base.py\", line 1389, in wrapper\n",
            "    return fit_method(estimator, *args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py\", line 1193, in fit\n",
            "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py\", line 63, in _check_solver\n",
            "    raise ValueError(\n",
            "ValueError: Solver lbfgs supports only 'l2' or None penalties, got elasticnet penalty.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "3 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/base.py\", line 1389, in wrapper\n",
            "    return fit_method(estimator, *args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py\", line 1193, in fit\n",
            "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py\", line 71, in _check_solver\n",
            "    raise ValueError(\n",
            "ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_search.py:1108: UserWarning: One or more of the test scores are non-finite: [       nan        nan        nan 0.89125435        nan        nan\n",
            "        nan 0.88875747        nan 0.89250279        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan]\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#10 Write a Python program to implement One-vs-One (OvO) Multiclass Logistic Regression and print the accuracy.\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.multiclass import OneVsOneClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the dataset\n",
        "dataset = pd.read_csv('/content/large_logistic_data.csv')\n",
        "\n",
        "# Define the feature columns and target column\n",
        "X = dataset[['age', 'salary', 'education_level']]\n",
        "y = dataset['purchased']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize Logistic Regression model\n",
        "logreg = LogisticRegression(max_iter=1000)\n",
        "\n",
        "# Apply One-vs-One (OvO) multiclass strategy\n",
        "ovo_classifier = OneVsOneClassifier(logreg)\n",
        "\n",
        "# Fit the model on the training data\n",
        "ovo_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = ovo_classifier.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print the accuracy\n",
        "print(f\"Accuracy of OvO Logistic Regression: {accuracy * 100:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S5krq24uTK-9",
        "outputId": "77a1d046-1b9e-4d59-9fcf-406ab5ad927c"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of OvO Logistic Regression: 91.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#11. Write a Python program to train a Logistic Regression model and visualize the confusion matrix for binary classification.\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "\n",
        "# Load the dataset\n",
        "dataset = pd.read_csv('/content/large_logistic_data.csv')\n",
        "\n",
        "# Define the feature columns (X) and target column (y)\n",
        "X = dataset[['age', 'salary', 'education_level']]\n",
        "y = dataset['purchased']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize Logistic Regression model\n",
        "logreg = LogisticRegression(max_iter=1000)\n",
        "\n",
        "# Train the model on the training data\n",
        "logreg.fit(X_train, y_train)\n",
        "\n",
        "# Predict the results on the test set\n",
        "y_pred = logreg.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy of Logistic Regression: {accuracy * 100:.2f}%\")\n",
        "\n",
        "# Generate the confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Visualize the confusion matrix using a heatmap\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False, xticklabels=['Not Purchased', 'Purchased'], yticklabels=['Not Purchased', 'Purchased'])\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.title('Confusion Matrix for Binary Classification')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 504
        },
        "id": "P7z5Dr3ETLBn",
        "outputId": "cbb66308-a2ad-46c3-fc26-737f8963f252"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of Logistic Regression: 91.00%\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 600x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#12. Write a Python program to train a Logistic Regression model and evaluate its performance using Precision, Recall, and F1-Score.\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
        "\n",
        "# Load the dataset\n",
        "dataset = pd.read_csv('/content/large_logistic_data.csv')\n",
        "\n",
        "# Define the feature columns (X) and target column (y)\n",
        "X = dataset[['age', 'salary', 'education_level']]\n",
        "y = dataset['purchased']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize Logistic Regression model\n",
        "logreg = LogisticRegression(max_iter=1000)\n",
        "\n",
        "# Train the model on the training data\n",
        "logreg.fit(X_train, y_train)\n",
        "\n",
        "# Predict the results on the test set\n",
        "y_pred = logreg.predict(X_test)\n",
        "\n",
        "# Calculate Precision, Recall, and F1-Score\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "# Print the performance metrics\n",
        "print(f\"Accuracy of Logistic Regression: {accuracy_score(y_test, y_pred) * 100:.2f}%\")\n",
        "print(f\"Precision: {precision * 100:.2f}%\")\n",
        "print(f\"Recall: {recall * 100:.2f}%\")\n",
        "print(f\"F1-Score: {f1 * 100:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "snecE_M4TLEG",
        "outputId": "9de068f4-d8ab-4b07-a1b3-0d2a667b85e5"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of Logistic Regression: 91.00%\n",
            "Precision: 92.39%\n",
            "Recall: 98.38%\n",
            "F1-Score: 95.29%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#13. Write a Python program to train a Logistic Regression model on imbalanced data and apply class weights to improve model performance.\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Load the dataset\n",
        "dataset = pd.read_csv('/content/large_logistic_data.csv')\n",
        "\n",
        "# Define the feature columns (X) and target column (y)\n",
        "X = dataset[['age', 'salary', 'education_level']]\n",
        "y = dataset['purchased']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize Logistic Regression model with class weight adjustment\n",
        "logreg = LogisticRegression(max_iter=1000, class_weight='balanced')\n",
        "\n",
        "# Train the model on the training data\n",
        "logreg.fit(X_train, y_train)\n",
        "\n",
        "# Predict the results on the test set\n",
        "y_pred = logreg.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print the model's accuracy and classification report (precision, recall, f1-score)\n",
        "print(f\"Accuracy of Logistic Regression: {accuracy * 100:.2f}%\")\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jril2o9dTLHA",
        "outputId": "06fa5135-4c52-4178-95c9-8b38d874c529"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of Logistic Regression: 69.00%\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.14      0.60      0.23        15\n",
            "           1       0.96      0.70      0.81       185\n",
            "\n",
            "    accuracy                           0.69       200\n",
            "   macro avg       0.55      0.65      0.52       200\n",
            "weighted avg       0.89      0.69      0.76       200\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#14. Write a Python program to train a Logistic Regression model on the Titanic dataset, handle missing values, and evaluate its performance.\n",
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from sklearn.impute import SimpleImputer\n",
        "import seaborn as sns\n",
        "\n",
        "# Load the Titanic dataset\n",
        "def load_dataset():\n",
        "    \"\"\"Loads the Titanic dataset.\"\"\"\n",
        "    return sns.load_dataset('titanic')\n",
        "\n",
        "# Preprocess the dataset\n",
        "def preprocess_dataset(df):\n",
        "    \"\"\"Preprocesses the dataset by handling missing values and encoding categorical variables.\"\"\"\n",
        "    # Drop unnecessary columns\n",
        "    df = df.drop(['deck', 'embark_town', 'alive', 'class', 'who', 'adult_male', 'alone'], axis=1)\n",
        "\n",
        "    # Handle missing values in the 'age' column\n",
        "    imputer = SimpleImputer(strategy='mean')\n",
        "    df['age'] = imputer.fit_transform(df[['age']])\n",
        "\n",
        "    # Encode categorical variables\n",
        "    df['sex'] = df['sex'].map({'male': 0, 'female': 1})\n",
        "    df['embarked'] = df['embarked'].map({'S': 0, 'C': 1, 'Q': 2})\n",
        "\n",
        "    # Handle missing values in the 'embarked' column\n",
        "    df['embarked'] = df['embarked'].fillna(0)\n",
        "\n",
        "    return df\n",
        "\n",
        "# Train the Logistic Regression model\n",
        "def train_model(X_train, y_train):\n",
        "    \"\"\"Trains a Logistic Regression model.\"\"\"\n",
        "    # Standardize features\n",
        "    scaler = StandardScaler()\n",
        "    X_train = scaler.fit_transform(X_train)\n",
        "\n",
        "    # Train the model\n",
        "    model = LogisticRegression(max_iter=1000)\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    return model, scaler\n",
        "\n",
        "# Evaluate the model's performance\n",
        "def evaluate_model(model, X_test, y_test, scaler):\n",
        "    \"\"\"Evaluates the model's performance.\"\"\"\n",
        "    # Standardize features\n",
        "    X_test = scaler.transform(X_test)\n",
        "\n",
        "    # Make predictions\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    # Calculate accuracy\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "    # Calculate classification report\n",
        "    report = classification_report(y_test, y_pred)\n",
        "\n",
        "    # Calculate confusion matrix\n",
        "    matrix = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "    return accuracy, report, matrix\n",
        "\n",
        "# Main function\n",
        "def main():\n",
        "    # Load the dataset\n",
        "    df = load_dataset()\n",
        "\n",
        "    # Preprocess the dataset\n",
        "    df = preprocess_dataset(df)\n",
        "\n",
        "    # Split the dataset into features and target\n",
        "    X = df.drop('survived', axis=1)\n",
        "    y = df['survived']\n",
        "\n",
        "    # Split the dataset into training and testing sets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Train the model\n",
        "    model, scaler = train_model(X_train, y_train)\n",
        "\n",
        "    # Evaluate the model's performance\n",
        "    accuracy, report, matrix = evaluate_model(model, X_test, y_test, scaler)\n",
        "\n",
        "    # Print the results\n",
        "    print(\"Accuracy:\", accuracy)\n",
        "    print(\"Classification Report:\\n\", report)\n",
        "    print(\"Confusion Matrix:\\n\", matrix)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O90bAGc1TLJr",
        "outputId": "9f358bb8-4284-4917-ce9e-fd061f21be9b"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.7988826815642458\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.85      0.83       105\n",
            "           1       0.77      0.73      0.75        74\n",
            "\n",
            "    accuracy                           0.80       179\n",
            "   macro avg       0.79      0.79      0.79       179\n",
            "weighted avg       0.80      0.80      0.80       179\n",
            "\n",
            "Confusion Matrix:\n",
            " [[89 16]\n",
            " [20 54]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#15. Write a Python program to apply feature scaling (Standardization) before training a Logistic Regression model. Evaluate its accuracy and compare results with and without scaling.\n",
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# Load the Iris dataset\n",
        "def load_dataset():\n",
        "    \"\"\"Loads the Iris dataset.\"\"\"\n",
        "    iris = load_iris()\n",
        "    df = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n",
        "    df['target'] = iris.target\n",
        "    return df\n",
        "\n",
        "# Split the dataset into features and target\n",
        "def split_dataset(df):\n",
        "    \"\"\"Splits the dataset into features and target.\"\"\"\n",
        "    X = df.drop('target', axis=1)\n",
        "    y = df['target']\n",
        "    return X, y\n",
        "\n",
        "# Train a Logistic Regression model without scaling\n",
        "def train_model_without_scaling(X_train, y_train):\n",
        "    \"\"\"Trains a Logistic Regression model without scaling.\"\"\"\n",
        "    model = LogisticRegression(max_iter=1000)\n",
        "    model.fit(X_train, y_train)\n",
        "    return model\n",
        "\n",
        "# Train a Logistic Regression model with scaling\n",
        "def train_model_with_scaling(X_train, y_train):\n",
        "    \"\"\"Trains a Logistic Regression model with scaling.\"\"\"\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    model = LogisticRegression(max_iter=1000)\n",
        "    model.fit(X_train_scaled, y_train)\n",
        "    return model, scaler\n",
        "\n",
        "# Evaluate the model's performance\n",
        "def evaluate_model(model, X_test, y_test, scaler=None):\n",
        "    \"\"\"Evaluates the model's performance.\"\"\"\n",
        "    if scaler:\n",
        "        X_test_scaled = scaler.transform(X_test)\n",
        "        y_pred = model.predict(X_test_scaled)\n",
        "    else:\n",
        "        y_pred = model.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    return accuracy\n",
        "\n",
        "# Main function\n",
        "def main():\n",
        "    # Load the dataset\n",
        "    df = load_dataset()\n",
        "\n",
        "    # Split the dataset into features and target\n",
        "    X, y = split_dataset(df)\n",
        "\n",
        "    # Split the dataset into training and testing sets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Train a model without scaling\n",
        "    model_without_scaling = train_model_without_scaling(X_train, y_train)\n",
        "    accuracy_without_scaling = evaluate_model(model_without_scaling, X_test, y_test)\n",
        "    print(\"Accuracy without scaling:\", accuracy_without_scaling)\n",
        "\n",
        "    # Train a model with scaling\n",
        "    model_with_scaling, scaler = train_model_with_scaling(X_train, y_train)\n",
        "    accuracy_with_scaling = evaluate_model(model_with_scaling, X_test, y_test, scaler)\n",
        "    print(\"Accuracy with scaling:\", accuracy_with_scaling)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n4pUiGVDTLMf",
        "outputId": "b9e6f38b-2662-4d65-9f98-4255624f46c4"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy without scaling: 1.0\n",
            "Accuracy with scaling: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#16. Write a Python program to train Logistic Regression and evaluate its performance using ROC-AUC score.\n",
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score, roc_curve, auc\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "def load_dataset():\n",
        "    \"\"\"Loads the Breast Cancer dataset.\"\"\"\n",
        "    breast_cancer = load_breast_cancer()\n",
        "    df = pd.DataFrame(data=breast_cancer.data, columns=breast_cancer.feature_names)\n",
        "    df['target'] = breast_cancer.target\n",
        "    return df\n",
        "\n",
        "# Split the dataset into features and target\n",
        "def split_dataset(df):\n",
        "    \"\"\"Splits the dataset into features and target.\"\"\"\n",
        "    X = df.drop('target', axis=1)\n",
        "    y = df['target']\n",
        "    return X, y\n",
        "\n",
        "# Train a Logistic Regression model\n",
        "def train_model(X_train, y_train):\n",
        "    \"\"\"Trains a Logistic Regression model.\"\"\"\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    model = LogisticRegression(max_iter=1000)\n",
        "    model.fit(X_train_scaled, y_train)\n",
        "    return model, scaler\n",
        "\n",
        "# Evaluate the model's performance using ROC-AUC score\n",
        "def evaluate_model(model, X_test, y_test, scaler):\n",
        "    \"\"\"Evaluates the model's performance using ROC-AUC score.\"\"\"\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "    y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]\n",
        "    auc_score = roc_auc_score(y_test, y_pred_proba)\n",
        "    return auc_score, y_pred_proba\n",
        "\n",
        "# Plot the ROC curve\n",
        "def plot_roc_curve(y_test, y_pred_proba):\n",
        "    \"\"\"Plots the ROC curve.\"\"\"\n",
        "    fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
        "    auc_score = auc(fpr, tpr)\n",
        "    plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % auc_score)\n",
        "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('Receiver operating characteristic')\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.show()\n",
        "\n",
        "# Main function\n",
        "def main():\n",
        "    # Load the dataset\n",
        "    df = load_dataset()\n",
        "\n",
        "    # Split the dataset into features and target\n",
        "    X, y = split_dataset(df)\n",
        "\n",
        "    # Split the dataset into training and testing sets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Train a Logistic Regression model\n",
        "    model, scaler = train_model(X_train, y_train)\n",
        "\n",
        "    # Evaluate the model's performance using ROC-AUC score\n",
        "    auc_score, y_pred_proba = evaluate_model(model, X_test, y_test, scaler)\n",
        "    print(\"ROC-AUC Score:\", auc_score)\n",
        "\n",
        "    # Plot the ROC curve\n",
        "    plot_roc_curve(y_test, y_pred_proba)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 489
        },
        "id": "Yc95-tK7TLPV",
        "outputId": "36a0fe73-e554-4983-ebce-a866a425d745"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROC-AUC Score: 0.99737962659679\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#17. Write a Python program to train Logistic Regression using a custom learning rate (C=0.5) and evaluate accuracy.\n",
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# Load the Iris dataset\n",
        "def load_dataset():\n",
        "    \"\"\"Loads the Iris dataset.\"\"\"\n",
        "    iris = load_iris()\n",
        "    df = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n",
        "    df['target'] = iris.target\n",
        "    # Convert to binary classification problem\n",
        "    df['target'] = np.where(df['target'] > 0, 1, 0)\n",
        "    return df\n",
        "\n",
        "# Split the dataset into features and target\n",
        "def split_dataset(df):\n",
        "    \"\"\"Splits the dataset into features and target.\"\"\"\n",
        "    X = df.drop('target', axis=1)\n",
        "    y = df['target']\n",
        "    return X, y\n",
        "\n",
        "# Train a Logistic Regression model\n",
        "def train_model(X_train, y_train):\n",
        "    \"\"\"Trains a Logistic Regression model.\"\"\"\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    model = LogisticRegression(C=0.5, max_iter=1000)\n",
        "    model.fit(X_train_scaled, y_train)\n",
        "    return model, scaler\n",
        "\n",
        "# Evaluate the model's performance\n",
        "def evaluate_model(model, X_test, y_test, scaler):\n",
        "    \"\"\"Evaluates the model's performance.\"\"\"\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "    y_pred = model.predict(X_test_scaled)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    return accuracy\n",
        "\n",
        "# Main function\n",
        "def main():\n",
        "    # Load the dataset\n",
        "    df = load_dataset()\n",
        "\n",
        "    # Split the dataset into features and target\n",
        "    X, y = split_dataset(df)\n",
        "\n",
        "    # Split the dataset into training and testing sets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Train a Logistic Regression model\n",
        "    model, scaler = train_model(X_train, y_train)\n",
        "\n",
        "    # Evaluate the model's performance\n",
        "    accuracy = evaluate_model(model, X_test, y_test, scaler)\n",
        "    print(\"Accuracy:\", accuracy)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F7pznljwTLSH",
        "outputId": "38efc24a-ff4e-4f15-d316-8e081cf67c0e"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#18. Write a Python program to train Logistic Regression and identify important features based on model coefficients.\n",
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "def load_dataset():\n",
        "    \"\"\"Loads the Breast Cancer dataset.\"\"\"\n",
        "    breast_cancer = load_breast_cancer()\n",
        "    df = pd.DataFrame(data=breast_cancer.data, columns=breast_cancer.feature_names)\n",
        "    df['target'] = breast_cancer.target\n",
        "    return df\n",
        "\n",
        "# Split the dataset into features and target\n",
        "def split_dataset(df):\n",
        "    \"\"\"Splits the dataset into features and target.\"\"\"\n",
        "    X = df.drop('target', axis=1)\n",
        "    y = df['target']\n",
        "    return X, y\n",
        "\n",
        "# Train a Logistic Regression model\n",
        "def train_model(X_train, y_train):\n",
        "    \"\"\"Trains a Logistic Regression model.\"\"\"\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    model = LogisticRegression(max_iter=1000)\n",
        "    model.fit(X_train_scaled, y_train)\n",
        "    return model, scaler\n",
        "\n",
        "# Identify important features based on model coefficients\n",
        "def identify_important_features(model, X_train):\n",
        "    \"\"\"Identifies important features based on model coefficients.\"\"\"\n",
        "    feature_importances = model.coef_[0]\n",
        "    feature_names = X_train.columns\n",
        "    feature_importances_df = pd.DataFrame({'Feature': feature_names, 'Importance': feature_importances})\n",
        "    feature_importances_df = feature_importances_df.sort_values(by='Importance', ascending=False)\n",
        "    return feature_importances_df\n",
        "\n",
        "# Main function\n",
        "def main():\n",
        "    # Load the dataset\n",
        "    df = load_dataset()\n",
        "\n",
        "    # Split the dataset into features and target\n",
        "    X, y = split_dataset(df)\n",
        "\n",
        "    # Split the dataset into training and testing sets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Train a Logistic Regression model\n",
        "    model, scaler = train_model(X_train, y_train)\n",
        "\n",
        "    # Identify important features based on model coefficients\n",
        "    feature_importances_df = identify_important_features(model, X_train)\n",
        "    print(\"Important Features:\")\n",
        "    print(feature_importances_df)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jfLEy2_yTLU5",
        "outputId": "1ab10844-aca9-4057-d357-f13d5651d50b"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Important Features:\n",
            "                    Feature  Importance\n",
            "15        compactness error    0.682491\n",
            "19  fractal dimension error    0.616230\n",
            "5          mean compactness    0.540164\n",
            "18           symmetry error    0.500425\n",
            "8             mean symmetry    0.236119\n",
            "11            texture error    0.188877\n",
            "16          concavity error    0.175275\n",
            "9    mean fractal dimension    0.075921\n",
            "25        worst compactness    0.016110\n",
            "4           mean smoothness   -0.071667\n",
            "29  worst fractal dimension   -0.157414\n",
            "17     concave points error   -0.311300\n",
            "14         smoothness error   -0.313307\n",
            "1              mean texture   -0.387326\n",
            "2            mean perimeter   -0.393432\n",
            "0               mean radius   -0.431904\n",
            "3                 mean area   -0.465210\n",
            "24         worst smoothness   -0.544170\n",
            "22          worst perimeter   -0.589453\n",
            "12          perimeter error   -0.610583\n",
            "27     worst concave points   -0.778217\n",
            "6            mean concavity   -0.801458\n",
            "23               worst area   -0.841846\n",
            "20             worst radius   -0.879840\n",
            "13               area error   -0.907186\n",
            "26          worst concavity   -0.943053\n",
            "7       mean concave points   -1.119804\n",
            "28           worst symmetry   -1.208200\n",
            "10             radius error   -1.268178\n",
            "21            worst texture   -1.350606\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#19. Write a Python program to train Logistic Regression and evaluate its performance using Cohen's Kappa Score.\n",
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "def load_dataset():\n",
        "    \"\"\"Loads the Breast Cancer dataset.\"\"\"\n",
        "    breast_cancer = load_breast_cancer()\n",
        "    df = pd.DataFrame(data=breast_cancer.data, columns=breast_cancer.feature_names)\n",
        "    df['target'] = breast_cancer.target\n",
        "    return df\n",
        "\n",
        "# Split the dataset into features and target\n",
        "def split_dataset(df):\n",
        "    \"\"\"Splits the dataset into features and target.\"\"\"\n",
        "    X = df.drop('target', axis=1)\n",
        "    y = df['target']\n",
        "    return X, y\n",
        "\n",
        "# Train a Logistic Regression model\n",
        "def train_model(X_train, y_train):\n",
        "    \"\"\"Trains a Logistic Regression model.\"\"\"\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    model = LogisticRegression(max_iter=1000)\n",
        "    model.fit(X_train_scaled, y_train)\n",
        "    return model, scaler\n",
        "\n",
        "# Evaluate the model's performance using Cohen's Kappa Score\n",
        "def evaluate_model(model, X_test, y_test, scaler):\n",
        "    \"\"\"Evaluates the model's performance using Cohen's Kappa Score.\"\"\"\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "    y_pred = model.predict(X_test_scaled)\n",
        "    kappa_score = cohen_kappa_score(y_test, y_pred)\n",
        "    return kappa_score\n",
        "\n",
        "# Main function\n",
        "def main():\n",
        "    # Load the dataset\n",
        "    df = load_dataset()\n",
        "\n",
        "    # Split the dataset into features and target\n",
        "    X, y = split_dataset(df)\n",
        "\n",
        "    # Split the dataset into training and testing sets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Train a Logistic Regression model\n",
        "    model, scaler = train_model(X_train, y_train)\n",
        "\n",
        "    # Evaluate the model's performance using Cohen's Kappa Score\n",
        "    kappa_score = evaluate_model(model, X_test, y_test, scaler)\n",
        "    print(\"Cohen's Kappa Score:\", kappa_score)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pMAZs1gNTLXz",
        "outputId": "0a283aa8-6ce1-44a1-ceba-97cb47f570cf"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cohen's Kappa Score: 0.9437314906219151\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#20. Write a Python program to train Logistic Regression and visualize the Precision-Recall Curve for binary classification.\n",
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import precision_recall_curve, auc\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "def load_dataset():\n",
        "    \"\"\"Loads the Breast Cancer dataset.\"\"\"\n",
        "    breast_cancer = load_breast_cancer()\n",
        "    df = pd.DataFrame(data=breast_cancer.data, columns=breast_cancer.feature_names)\n",
        "    df['target'] = breast_cancer.target\n",
        "    return df\n",
        "\n",
        "# Split the dataset into features and target\n",
        "def split_dataset(df):\n",
        "    \"\"\"Splits the dataset into features and target.\"\"\"\n",
        "    X = df.drop('target', axis=1)\n",
        "    y = df['target']\n",
        "    return X, y\n",
        "\n",
        "# Train a Logistic Regression model\n",
        "def train_model(X_train, y_train):\n",
        "    \"\"\"Trains a Logistic Regression model.\"\"\"\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    model = LogisticRegression(max_iter=1000)\n",
        "    model.fit(X_train_scaled, y_train)\n",
        "    return model, scaler\n",
        "\n",
        "# Plot the Precision-Recall Curve\n",
        "def plot_precision_recall_curve(model, X_test, y_test, scaler):\n",
        "    \"\"\"Plots the Precision-Recall Curve.\"\"\"\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "    y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]\n",
        "    precision, recall, _ = precision_recall_curve(y_test, y_pred_proba)\n",
        "    auc_score = auc(recall, precision)\n",
        "    plt.plot(recall, precision, color='darkorange', lw=2, label='Precision-Recall Curve (AUC = %0.2f)' % auc_score)\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('Recall')\n",
        "    plt.ylabel('Precision')\n",
        "    plt.title('Precision-Recall Curve')\n",
        "    plt.legend(loc=\"lower left\")\n",
        "    plt.show()\n",
        "\n",
        "# Main function\n",
        "def main():\n",
        "    # Load the dataset\n",
        "    df = load_dataset()\n",
        "\n",
        "    # Split the dataset into features and target\n",
        "    X, y = split_dataset(df)\n",
        "\n",
        "    # Split the dataset into training and testing sets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Train a Logistic Regression model\n",
        "    model, scaler = train_model(X_train, y_train)\n",
        "\n",
        "    # Plot the Precision-Recall Curve\n",
        "    plot_precision_recall_curve(model, X_test, y_test, scaler)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "gfAxr3k7TLbc",
        "outputId": "5f7b6a7d-4ef2-48d9-b633-fe2db2305291"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#21. Write a Python program to train Logistic Regression with different solvers (liblinear, saga, lbfgs) and compare their accuracy.\n",
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "def load_dataset():\n",
        "    \"\"\"Loads the Breast Cancer dataset.\"\"\"\n",
        "    breast_cancer = load_breast_cancer()\n",
        "    df = pd.DataFrame(data=breast_cancer.data, columns=breast_cancer.feature_names)\n",
        "    df['target'] = breast_cancer.target\n",
        "    return df\n",
        "\n",
        "# Split the dataset into features and target\n",
        "def split_dataset(df):\n",
        "    \"\"\"Splits the dataset into features and target.\"\"\"\n",
        "    X = df.drop('target', axis=1)\n",
        "    y = df['target']\n",
        "    return X, y\n",
        "\n",
        "# Train a Logistic Regression model with different solvers\n",
        "def train_model(X_train, y_train, solver):\n",
        "    \"\"\"Trains a Logistic Regression model with a specified solver.\"\"\"\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    model = LogisticRegression(max_iter=1000, solver=solver)\n",
        "    model.fit(X_train_scaled, y_train)\n",
        "    return model, scaler\n",
        "\n",
        "# Evaluate the model's performance\n",
        "def evaluate_model(model, X_test, y_test, scaler):\n",
        "    \"\"\"Evaluates the model's performance.\"\"\"\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "    y_pred = model.predict(X_test_scaled)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    return accuracy\n",
        "\n",
        "# Main function\n",
        "def main():\n",
        "    # Load the dataset\n",
        "    df = load_dataset()\n",
        "\n",
        "    # Split the dataset into features and target\n",
        "    X, y = split_dataset(df)\n",
        "\n",
        "    # Split the dataset into training and testing sets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Define solvers to compare\n",
        "    solvers = ['liblinear', 'saga', 'lbfgs']\n",
        "\n",
        "    # Train and evaluate models with different solvers\n",
        "    for solver in solvers:\n",
        "        model, scaler = train_model(X_train, y_train, solver)\n",
        "        accuracy = evaluate_model(model, X_test, y_test, scaler)\n",
        "        print(f\"Solver: {solver}, Accuracy: {accuracy:.3f}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BcxdbXryTLfd",
        "outputId": "3462979a-dbe6-4f6f-f9f0-84652f39ba11"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Solver: liblinear, Accuracy: 0.974\n",
            "Solver: saga, Accuracy: 0.974\n",
            "Solver: lbfgs, Accuracy: 0.974\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#22. Write a Python program to train Logistic Regression and evaluate its performance using Matthews Correlation Coefficient (MCC).\n",
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "def load_dataset():\n",
        "    \"\"\"Loads the Breast Cancer dataset.\"\"\"\n",
        "    breast_cancer = load_breast_cancer()\n",
        "    df = pd.DataFrame(data=breast_cancer.data, columns=breast_cancer.feature_names)\n",
        "    df['target'] = breast_cancer.target\n",
        "    return df\n",
        "\n",
        "# Split the dataset into features and target\n",
        "def split_dataset(df):\n",
        "    \"\"\"Splits the dataset into features and target.\"\"\"\n",
        "    X = df.drop('target', axis=1)\n",
        "    y = df['target']\n",
        "    return X, y\n",
        "\n",
        "# Train a Logistic Regression model\n",
        "def train_model(X_train, y_train):\n",
        "    \"\"\"Trains a Logistic Regression model.\"\"\"\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    model = LogisticRegression(max_iter=1000)\n",
        "    model.fit(X_train_scaled, y_train)\n",
        "    return model, scaler\n",
        "\n",
        "# Evaluate the model's performance using Matthews Correlation Coefficient (MCC)\n",
        "def evaluate_model(model, X_test, y_test, scaler):\n",
        "    \"\"\"Evaluates the model's performance using MCC.\"\"\"\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "    y_pred = model.predict(X_test_scaled)\n",
        "    mcc = matthews_corrcoef(y_test, y_pred)\n",
        "    return mcc\n",
        "\n",
        "# Main function\n",
        "def main():\n",
        "    # Load the dataset\n",
        "    df = load_dataset()\n",
        "\n",
        "    # Split the dataset into features and target\n",
        "    X, y = split_dataset(df)\n",
        "\n",
        "    # Split the dataset into training and testing sets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Train a Logistic Regression model\n",
        "    model, scaler = train_model(X_train, y_train)\n",
        "\n",
        "    # Evaluate the model's performance using MCC\n",
        "    mcc = evaluate_model(model, X_test, y_test, scaler)\n",
        "    print(\"Matthews Correlation Coefficient (MCC):\", mcc)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3zjJ_7lUTLjS",
        "outputId": "26446c8c-5cae-48fd-d9cd-34a2f17a16d5"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matthews Correlation Coefficient (MCC): 0.9438975339262832\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#23. Write a Python program to train Logistic Regression on both raw and standardized data. Compare their accuracy to see the impact of feature scaling.\n",
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "def load_dataset():\n",
        "    \"\"\"Loads the Breast Cancer dataset.\"\"\"\n",
        "    breast_cancer = load_breast_cancer()\n",
        "    df = pd.DataFrame(data=breast_cancer.data, columns=breast_cancer.feature_names)\n",
        "    df['target'] = breast_cancer.target\n",
        "    return df\n",
        "\n",
        "# Split the dataset into features and target\n",
        "def split_dataset(df):\n",
        "    \"\"\"Splits the dataset into features and target.\"\"\"\n",
        "    X = df.drop('target', axis=1)\n",
        "    y = df['target']\n",
        "    return X, y\n",
        "\n",
        "# Train a Logistic Regression model\n",
        "def train_model(X_train, y_train):\n",
        "    \"\"\"Trains a Logistic Regression model.\"\"\"\n",
        "    model = LogisticRegression(max_iter=1000)\n",
        "    model.fit(X_train, y_train)\n",
        "    return model\n",
        "\n",
        "# Evaluate the model's performance\n",
        "def evaluate_model(model, X_test, y_test):\n",
        "    \"\"\"Evaluates the model's performance.\"\"\"\n",
        "    y_pred = model.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    return accuracy\n",
        "\n",
        "# Main function\n",
        "def main():\n",
        "    # Load the dataset\n",
        "    df = load_dataset()\n",
        "\n",
        "    # Split the dataset into features and target\n",
        "    X, y = split_dataset(df)\n",
        "\n",
        "    # Split the dataset into training and testing sets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Train a Logistic Regression model on raw data\n",
        "    model_raw = train_model(X_train, y_train)\n",
        "    accuracy_raw = evaluate_model(model_raw, X_test, y_test)\n",
        "    print(\"Accuracy on raw data:\", accuracy_raw)\n",
        "\n",
        "    # Standardize the data\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "    # Train a Logistic Regression model on standardized data\n",
        "    model_scaled = train_model(X_train_scaled, y_train)\n",
        "    accuracy_scaled = evaluate_model(model_scaled, X_test_scaled, y_test)\n",
        "    print(\"Accuracy on standardized data:\", accuracy_scaled)\n",
        "\n",
        "    # Compare the accuracy\n",
        "    print(\"Difference in accuracy:\", accuracy_scaled - accuracy_raw)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2OG-DnirTLlY",
        "outputId": "2f847a71-6dfe-4d82-ea2b-f7782c6549dc"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on raw data: 0.956140350877193\n",
            "Accuracy on standardized data: 0.9736842105263158\n",
            "Difference in accuracy: 0.01754385964912286\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#24. Write a Python program to train Logistic Regression and find the optimal C (regularization strength) using cross-validation.\n",
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "def load_dataset():\n",
        "    \"\"\"Loads the Breast Cancer dataset.\"\"\"\n",
        "    breast_cancer = load_breast_cancer()\n",
        "    df = pd.DataFrame(data=breast_cancer.data, columns=breast_cancer.feature_names)\n",
        "    df['target'] = breast_cancer.target\n",
        "    return df\n",
        "\n",
        "# Split the dataset into features and target\n",
        "def split_dataset(df):\n",
        "    \"\"\"Splits the dataset into features and target.\"\"\"\n",
        "    X = df.drop('target', axis=1)\n",
        "    y = df['target']\n",
        "    return X, y\n",
        "\n",
        "# Find the optimal C using cross-validation\n",
        "def find_optimal_c(X_train, y_train):\n",
        "    \"\"\"Finds the optimal C using cross-validation.\"\"\"\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    param_grid = {'C': [0.01, 0.1, 1, 10, 100]}\n",
        "    model = LogisticRegression(max_iter=1000)\n",
        "    grid_search = GridSearchCV(model, param_grid, cv=5)\n",
        "    grid_search.fit(X_train_scaled, y_train)\n",
        "    return grid_search.best_params_['C'], grid_search.best_score_\n",
        "\n",
        "# Train a Logistic Regression model with the optimal C\n",
        "def train_model(X_train, y_train, C):\n",
        "    \"\"\"Trains a Logistic Regression model with the optimal C.\"\"\"\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    model = LogisticRegression(max_iter=1000, C=C)\n",
        "    model.fit(X_train_scaled, y_train)\n",
        "    return model, scaler\n",
        "\n",
        "# Evaluate the model's performance\n",
        "def evaluate_model(model, X_test, y_test, scaler):\n",
        "    \"\"\"Evaluates the model's performance.\"\"\"\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "    y_pred = model.predict(X_test_scaled)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    return accuracy\n",
        "\n",
        "# Main function\n",
        "def main():\n",
        "    # Load the dataset\n",
        "    df = load_dataset()\n",
        "\n",
        "    # Split the dataset into features and target\n",
        "    X, y = split_dataset(df)\n",
        "\n",
        "    # Split the dataset into training and testing sets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Find the optimal C using cross-validation\n",
        "    optimal_c, best_score = find_optimal_c(X_train, y_train)\n",
        "    print(\"Optimal C:\", optimal_c)\n",
        "    print(\"Best cross-validation score:\", best_score)\n",
        "\n",
        "    # Train a Logistic Regression model with the optimal C\n",
        "    model, scaler = train_model(X_train, y_train, optimal_c)\n",
        "\n",
        "    # Evaluate the model's performance\n",
        "    accuracy = evaluate_model(model, X_test, y_test, scaler)\n",
        "    print(\"Accuracy on test set:\", accuracy)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J627y-UCTLn7",
        "outputId": "31021d10-e951-493a-8418-123ec0b2396d"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal C: 10\n",
            "Best cross-validation score: 0.9758241758241759\n",
            "Accuracy on test set: 0.9736842105263158\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#25. Write a Python program to train Logistic Regression, save the trained model using joblib, and load it again to make predictions.\n",
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "import joblib\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "def load_dataset():\n",
        "    \"\"\"Loads the Breast Cancer dataset.\"\"\"\n",
        "    breast_cancer = load_breast_cancer()\n",
        "    df = pd.DataFrame(data=breast_cancer.data, columns=breast_cancer.feature_names)\n",
        "    df['target'] = breast_cancer.target\n",
        "    return df\n",
        "\n",
        "# Split the dataset into features and target\n",
        "def split_dataset(df):\n",
        "    \"\"\"Splits the dataset into features and target.\"\"\"\n",
        "    X = df.drop('target', axis=1)\n",
        "    y = df['target']\n",
        "    return X, y\n",
        "\n",
        "# Train a Logistic Regression model\n",
        "def train_model(X_train, y_train):\n",
        "    \"\"\"Trains a Logistic Regression model.\"\"\"\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    model = LogisticRegression(max_iter=1000)\n",
        "    model.fit(X_train_scaled, y_train)\n",
        "    return model, scaler\n",
        "\n",
        "# Save the trained model using joblib\n",
        "def save_model(model, scaler, filename):\n",
        "    \"\"\"Saves the trained model using joblib.\"\"\"\n",
        "    joblib.dump((model, scaler), filename)\n",
        "\n",
        "# Load the saved model using joblib\n",
        "def load_model(filename):\n",
        "    \"\"\"Loads the saved model using joblib.\"\"\"\n",
        "    return joblib.load(filename)\n",
        "\n",
        "# Make predictions using the loaded model\n",
        "def make_predictions(model, scaler, X_test):\n",
        "    \"\"\"Makes predictions using the loaded model.\"\"\"\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "    return model.predict(X_test_scaled)\n",
        "\n",
        "# Main function\n",
        "def main():\n",
        "    # Load the dataset\n",
        "    df = load_dataset()\n",
        "\n",
        "    # Split the dataset into features and target\n",
        "    X, y = split_dataset(df)\n",
        "\n",
        "    # Split the dataset into training and testing sets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Train a Logistic Regression model\n",
        "    model, scaler = train_model(X_train, y_train)\n",
        "\n",
        "    # Save the trained model\n",
        "    filename = 'logistic_regression_model.joblib'\n",
        "    save_model(model, scaler, filename)\n",
        "    print(\"Model saved to\", filename)\n",
        "\n",
        "    # Load the saved model\n",
        "    loaded_model, loaded_scaler = load_model(filename)\n",
        "    print(\"Model loaded from\", filename)\n",
        "\n",
        "    # Make predictions using the loaded model\n",
        "    predictions = make_predictions(loaded_model, loaded_scaler, X_test)\n",
        "    print(\"Predictions:\", predictions)\n",
        "\n",
        "    # Evaluate the model's performance\n",
        "    accuracy = accuracy_score(y_test, predictions)\n",
        "    print(\"Accuracy:\", accuracy)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r8OyYCyHTLqE",
        "outputId": "1f788ced-198a-43bf-a490-1b2f06f55ec8"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved to logistic_regression_model.joblib\n",
            "Model loaded from logistic_regression_model.joblib\n",
            "Predictions: [1 0 0 1 1 0 0 0 1 1 1 0 1 0 1 0 1 1 1 0 1 1 0 1 1 1 1 1 1 0 1 1 1 1 1 1 0\n",
            " 1 0 1 1 0 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 0 0 1 1 0 0 1 1 1 0 0 1 1 0 0 1 0\n",
            " 1 1 1 1 1 1 0 1 0 0 0 0 0 0 1 1 1 1 1 1 1 1 0 0 1 0 0 1 0 0 1 1 1 0 1 1 0\n",
            " 1 0 0]\n",
            "Accuracy: 0.9736842105263158\n"
          ]
        }
      ]
    }
  ]
}